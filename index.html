<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>EVN PPE Monitor v2.24</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <style>
        body { margin: 0; background: #000; overflow: hidden; font-family: sans-serif; }
        #container { position: relative; width: 100vw; height: 100vh; }
        video { width: 100%; height: 100%; object-fit: cover; }
        canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; z-index: 5; }
        .status-overlay { position: absolute; top: 15px; width: 100%; z-index: 10; text-align: center; }
        #status-label { background: rgba(0, 255, 0, 0.2); color: #0f0; padding: 6px 18px; border-radius: 20px; border: 1.5px solid #0f0; font-weight: bold; font-size: 13px; }
    </style>
</head>
<body>

<div id="container">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
    <div class="status-overlay"><span id="status-label">üîÑ ƒêANG KH·ªûI T·∫†O AI...</span></div>
</div>

<script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const statusLabel = document.getElementById('status-label');
    
    // TH·ª® T·ª∞ CLASS PPE C·ª¶A B·∫†N (S·ª≠a l·∫°i cho ƒë√∫ng v·ªõi file .pt)
    const labels = ["N√≥n b·∫£o h·ªô", "√Åo ph·∫£n quang", "Gi√†y b·∫£o h·ªô"]; 

    let session;

    async function start() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { facingMode: "environment", width: 640, height: 640 }
            });
            video.srcObject = stream;
            await new Promise(r => video.onloadedmetadata = r);

            // N·∫°p m√¥ h√¨nh ONNX
            session = await ort.InferenceSession.create('./best.onnx', { executionProviders: ['wasm'] });
            statusLabel.innerText = "üü¢ AI ƒêANG QU√âT TR·ª∞C TI·∫æP";
            processVideo();
        } catch (e) {
            statusLabel.innerText = "‚ùå L·ªñI: " + e.message;
        }
    }

    async function processVideo() {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        try {
            // 1. Ti·ªÅn x·ª≠ l√Ω Tensor
            const input = await prepareTensor(canvas);
            
            // 2. Ch·∫°y Inference
            const outputs = await session.run({ images: input });
            const rawData = outputs[Object.keys(outputs)[0]].data;

            // 3. Gi·∫£i m√£ v√† v·∫Ω khung th·ª±c t·∫ø (B·ªè khung xanh ƒë·ª©ng y√™n c≈©)
            decodeYOLOv8(rawData);
        } catch (err) {
            console.error(err);
        }

        requestAnimationFrame(processVideo);
    }

    async function prepareTensor(source) {
        const offscreen = new OffscreenCanvas(640, 640);
        const octx = offscreen.getContext('2d');
        octx.drawImage(source, 0, 0, 640, 640);
        const imgData = octx.getImageData(0, 0, 640, 640).data;

        const floatData = new Float32Array(3 * 640 * 640);
        for (let i = 0; i < 640 * 640; i++) {
            floatData[i] = imgData[i * 4] / 255.0;           // R
            floatData[i + 409600] = imgData[i * 4 + 1] / 255.0; // G
            floatData[i + 819200] = imgData[i * 4 + 2] / 255.0; // B
        }
        return new ort.Tensor('float32', floatData, [1, 3, 640, 640]);
    }

    function decodeYOLOv8(data) {
        // YOLOv8 output: [1, 4 + num_classes, 8400]
        const numClasses = labels.length;
        const totalRows = 8400;

        for (let i = 0; i < totalRows; i++) {
            let maxConfidence = 0;
            let classIdx = -1;

            // T√¨m class c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t
            for (let c = 0; c < numClasses; c++) {
                const conf = data[(4 + c) * totalRows + i];
                if (conf > maxConfidence) {
                    maxConfidence = conf;
                    classIdx = c;
                }
            }

            // Ng∆∞·ª°ng l·ªçc vi ph·∫°m (tƒÉng l√™n 0.45 ƒë·ªÉ ch√≠nh x√°c)
            if (maxConfidence > 0.45) {
                const cx = data[0 * totalRows + i];
                const cy = data[1 * totalRows + i];
                const w = data[2 * totalRows + i];
                const h = data[3 * totalRows + i];

                // Scale t·ªça ƒë·ªô v·ªÅ m√†n h√¨nh ƒëi·ªán tho·∫°i
                const scaleX = canvas.width / 640;
                const scaleY = canvas.height / 640;
                
                const x = (cx - w / 2) * scaleX;
                const y = (cy - h / 2) * scaleY;
                const rectW = w * scaleX;
                const rectH = h * scaleY;

                drawDetection(x, y, rectW, rectH, labels[classIdx], maxConfidence);
            }
        }
    }

    function drawDetection(x, y, w, h, label, conf) {
        ctx.strokeStyle = "#00ff00";
        ctx.lineWidth = 3;
        ctx.strokeRect(x, y, w, h);
        
        ctx.fillStyle = "#00ff00";
        ctx.font = "bold 16px Arial";
        const text = `${label} ${Math.round(conf * 100)}%`;
        ctx.fillText(text, x, y > 20 ? y - 10 : 20);
    }

    start();
</script>
</body>
</html>
